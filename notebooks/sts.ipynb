{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6d18fd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gtts import gTTS\n",
    "import io\n",
    "import base64\n",
    "\n",
    "def text_to_speech_base64(text, lang=\"en\"):\n",
    "    if not text:\n",
    "        return None\n",
    "\n",
    "    # Generate speech\n",
    "    tts = gTTS(text=text, lang=lang)\n",
    "\n",
    "    # Store audio in memory\n",
    "    audio_data = io.BytesIO()\n",
    "    tts.write_to_fp(audio_data)\n",
    "    audio_data.seek(0)\n",
    "\n",
    "    # Encode as base64\n",
    "    audio_base64 = base64.b64encode(audio_data.read()).decode(\"utf-8\")\n",
    "    return audio_base64\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c77d1319",
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_str = text_to_speech_base64(\"Hello, this is a test.\")\n",
    "print(audio_str[:100])  # preview first 100 chars\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cb0fc5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cf6e335",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import io\n",
    "import os\n",
    "import base64\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "def speech_to_text(audio_base64: str) -> str:\n",
    "    \"\"\"\n",
    "    Convert base64-encoded audio into text using OpenAI Whisper API.\n",
    "    \"\"\"\n",
    "    if not audio_base64:\n",
    "        return None\n",
    "    \n",
    "    # Decode base64 to bytes\n",
    "    audio_bytes = base64.b64decode(audio_base64)\n",
    "\n",
    "    # Wrap bytes in a file-like object\n",
    "    audio_file = io.BytesIO(audio_bytes)\n",
    "    audio_file.name = \"audio.mp3\"  # Whisper needs a filename (mp3/wav)\n",
    "\n",
    "    # Transcribe\n",
    "    transcript = openai.audio.transcriptions.create(\n",
    "        model=\"whisper-1\",\n",
    "        file=audio_file\n",
    "    )\n",
    "    \n",
    "    return transcript.text\n",
    "\n",
    "\n",
    "# Example usage (with the audio_str you generated earlier)\n",
    "text = speech_to_text(audio_str)\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "81f258fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Fast Language Models: Why Speed Matters\n",
      "\n",
      "Language models (LMs) power everything from chatbots and code autocompletion to search engines and real‑time translation.  \n",
      "While accuracy and capability remain the headline goals, **speed**—the latency of a single inference and the throughput of a system—has become a strategic differentiator in almost every domain that uses LMs. Below is a “why it matters” primer, broken into the main forces that make fast models indispensable.\n",
      "\n",
      "| # | Key Driver | What It Means | Practical Impact |\n",
      "|---|------------|---------------|------------------|\n",
      "| 1 | **User Experience** | Low latency → a conversation feels natural, a translation feels instant. | Reduces bounce rates, keeps users engaged. |\n",
      "| 2 | **Scalability & Cost** | Faster inference = more queries per GPU core = lower infrastructure cost. | Pay‑per‑second billing on cloud is cheaper; you can serve more users for the same budget. |\n",
      "| 3 | **Energy & Carbon Footprint** | Less compute time = lower power draw. | Helps companies meet ESG targets and reduce operational expenses. |\n",
      "| 4 | **Edge & On‑Device Deployment** | Models must run in milliseconds on CPUs or mobile NPUs. | Enables offline assistants, privacy‑first solutions, and wide‑area coverage where connectivity is spotty. |\n",
      "| 5 | **Real‑time Systems** | Speech‑to‑text, live captioning, gaming, autonomous vehicles. | Latency below 30 ms is often mandatory; any delay can be a safety risk or degrade the experience. |\n",
      "| 6 | **Research & Development** | Quick experimentation loops speed up model iteration. | Faster prototyping → more innovation per month. |\n",
      "| 7 | **Competitive Differentiation** | Speed is a hard metric customers can see; it’s often a selling point. | Fast models win in marketplaces where multiple LMs offer similar accuracy. |\n",
      "\n",
      "---\n",
      "\n",
      "## How Speed Is Achieved\n",
      "\n",
      "Speed gains come from a combination of **model architecture**, **optimization tricks**, and **hardware**. The typical pipeline looks like:\n",
      "\n",
      "1. **Model Design**  \n",
      "   * Transformer variants (e.g., BigBird, Longformer) reduce self‑attention complexity.  \n",
      "   * Recurrent or convolutional backbones (e.g., ConvBERT) can be faster on certain hardware.  \n",
      "\n",
      "2. **Compression & Optimization**  \n",
      "   * **Knowledge Distillation** – train a small “student” LM to mimic a large teacher.  \n",
      "   * **Quantization** – 8‑bit or 4‑bit integer weights reduce memory traffic.  \n",
      "   * **Pruning / Sparsity** – remove weights that have minimal impact.  \n",
      "   * **Operator Fusion** – merge softmax + logit layers to reduce kernel launches.  \n",
      "\n",
      "3. **Hardware Acceleration**  \n",
      "   * GPUs (NVIDIA A100, RTX 4090) – massive parallelism for dense matrix ops.  \n",
      "   * TPUs (Google Cloud TPU v4) – specialized systolic arrays for 16‑bit ops.  \n",
      "   * NPUs / FPGAs on mobile / edge devices – low‑power, low‑latency compute.  \n",
      "\n",
      "4. **Software Stack**  \n",
      "   * Efficient inference frameworks: TensorRT, ONNX Runtime, Triton Inference Server.  \n",
      "   * Batch‑sized scheduling to keep pipelines full.  \n",
      "   * Mixed‑precision libraries (CUDA, ROCm) to leverage GPU capabilities.\n",
      "\n",
      "---\n",
      "\n",
      "## Real‑World Use Cases Highlighting Speed\n",
      "\n",
      "| Domain | Speed Requirement | Why It Matters |\n",
      "|--------|-------------------|----------------|\n",
      "| **Chatbots** | < 200 ms per turn | Keeps a conversation fluid; any lag feels robotic. |\n",
      "| **Machine Translation** | < 1 s per sentence | Used in live meetings, news streams. |\n",
      "| **Speech‑to‑Text** | < 30 ms per token | Allows real‑time captioning for accessibility. |\n",
      "| **Autonomous Driving** | < 10 ms for perception | Critical safety decision loop. |\n",
      "| **Search & Retrieval** | < 50 ms per query | Users expect instant answers. |\n",
      "| **Content Generation** | < 2 s for a paragraph | Powering creative tools, writing assistants. |\n",
      "| **On‑Device Assistants** | < 200 ms on CPU | Enables offline operation with privacy guarantees. |\n",
      "\n",
      "---\n",
      "\n",
      "## Trade‑offs to Watch For\n",
      "\n",
      "| Aspect | Speed vs. Accuracy | Typical Strategy |\n",
      "|--------|--------------------|------------------|\n",
      "| **Model Size** | Large models (175B GPT‑3) are accurate but slow. | Use distilled or smaller variants (e.g., GPT‑NeoX 2.7B). |\n",
      "| **Precision** | 32‑bit FP vs. 16‑bit FP16 vs. 8‑bit INT8. | FP16 gives ~2× speed; INT8 gives ~4× but may lose a few BLEU points. |\n",
      "| **Batch Size** | Larger batches improve throughput but increase latency. | Use dynamic batching or pipeline parallelism. |\n",
      "| **Hardware Choice** | GPUs excel at batch throughput; NPUs excel at low‑latency single‑token. | Match workload to hardware. |\n",
      "\n",
      "---\n",
      "\n",
      "## Why Fast Models Are a Competitive Edge\n",
      "\n",
      "1. **Monetization** – Many SaaS platforms charge per token. Faster inference lets you serve more tokens for the same infrastructure cost, improving margins.  \n",
      "2. **User Retention** – Latency is invisible but feels like the product “lives.” A slow assistant becomes a friction point.  \n",
      "3. **Data Privacy** – On‑device inference means data never leaves the device. Speed is a prerequisite for this.  \n",
      "4. **Regulatory Compliance** – Some regions require “real‑time” data handling (e.g., live court transcripts).  \n",
      "5. **Market Differentiation** – In a sea of similar‑capability models, speed can be the deciding factor for enterprise contracts.\n",
      "\n",
      "---\n",
      "\n",
      "## Bottom Line\n",
      "\n",
      "Fast language models are no longer a nice‑to‑have; they’re a **business‑critical asset**. They unlock:\n",
      "\n",
      "- **Superior user experience** through low latency.  \n",
      "- **Cost‑effective scaling** by reducing compute per inference.  \n",
      "- **Broader deployment** to edge and on‑device scenarios.  \n",
      "- **Faster innovation cycles** for researchers and developers.  \n",
      "\n",
      "By investing in model compression, efficient architectures, and the right hardware, companies can deliver cutting‑edge language capabilities at a fraction of the operational cost—making speed a key lever in the AI strategy.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "from groq import Groq\n",
    "\n",
    "client = Groq(\n",
    "    api_key=os.environ.get(\"GROQ_API_KEY\"),\n",
    ")\n",
    "\n",
    "chat_completion = client.chat.completions.create(\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Explain the importance of fast language models\",\n",
    "        }\n",
    "    ],\n",
    "    model=\"openai/gpt-oss-20b\",\n",
    ")\n",
    "\n",
    "print(chat_completion.choices[0].message.content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
